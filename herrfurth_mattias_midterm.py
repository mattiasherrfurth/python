# -*- coding: utf-8 -*-
"""HERRFURTH MATTIAS Midterm.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1qWYqVc1puaFq9PZ-YjgOE-I6B4JUrXq9
"""

import pandas as pd
import numpy as np
from sklearn import datasets

wine_data = datasets.load_wine()
wine_df = pd.DataFrame(data=np.c_[wine_data['data'], wine_data['target']],columns= list(wine_data['feature_names']) + ['target'])

wine_df.head()

"""# Wine Data:

- Print the first 10 lines of the data set.  What are the columns?  What type of data? 10
- Use a command to retrieve the mean, median, 1st and 3rd quartiles of each column.  What is the mean of 'malic_acid', what is the max of magnesium? 10 
- Plot box blots of all features for each class, which value has the highest median? The most variance? 10
- Use RobustScalar to put all features on a similar scale.  Replot the box plots. 10
- Do a scatter plot of flavanoids vs hue with each target value being a different color.  How separated is the data? 10
- Do a NMF decomposition (2 components) of the data.  What features contribute the most to each component? Create a scatter plot of components 1 vs 2 with each target value a different color.  How separated is the data?  10
- With original data, create a pipeline to with to scale the data a train model random forest model .  Use GridSearch to find the best model, try 10, 50 and 100 trees and both gini and entropy. 10
- For the random forest model best model report: the confusion matrix and the accuracy. Which items were most misclassified and how? 10
"""

# Print the first 10 lines of the data set. What are the columns? What type of data?
wine_df.head(10)

wine_df.dtypes

# Use a command to retrieve the mean, median, 1st and 3rd quartiles of each column. What is the mean of 'malic_acid', what is the max of magnesium?
wine_df.describe()

# Plot box blots of all features for each class, which value has the highest median? The most variance?
wine_df.boxplot(rot=90)

# Use RobustScalar to put all features on a similar scale. Replot the box plots.
from sklearn.preprocessing import RobustScaler as RS
from sklearn.model_selection import train_test_split

X = wine_data.data
y = wine_data.target

scaleRS = RS()
fitsRS = scaleRS.fit(X)
wine_rs = pd.DataFrame(fitsRS.transform(X))
wine_rs['target'] = y

wine_rs.boxplot()

# Do a scatter plot of flavanoids vs hue with each target value being a different color. How separated is the data?
wine_df.plot.scatter(x='hue',y='flavanoids',c='target')

# Do a NMF decomposition (2 components) of the data. What features contribute the most to each component? 
from sklearn.decomposition import NMF

nmf = NMF(n_components=2, init='random', random_state=0)
W = nmf.fit_transform(X)
H = nmf.components_

print(H)

# Create a scatter plot of components 1 vs 2 with each target value a different color. How separated is the data?
w = pd.DataFrame(data=W,columns=['x_axis','y_axis'])
w['target'] = y

w.plot.scatter(x='x_axis',y='y_axis',c='target')

# With original data, create a pipeline to with to scale the data a train model random forest model . Use GridSearch to find the best model, try 10, 50 and 100 trees and both gini and entropy.
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=2)

from sklearn.ensemble import RandomForestClassifier as RFC
from sklearn.pipeline import Pipeline
from sklearn.model_selection import GridSearchCV

#train a random forest model with a robust scalar
pipeline = Pipeline(steps=[
        ('rs', RS()),
        ('rf', RFC())])

pipeline.fit(X_train, y_train)

#parameters = {'rs__with_centering':[True,False],'rs__with_scaling':[True,False],'rs__copy':[True,False],
#              'rf__n_estimators':[1,2,3,5,10],'rf__n_jobs':[1,2,3,5,10]}

parameters = {'rf__n_estimators':[10,50,100],'rf__criterion':['gini','entropy']}
search = GridSearchCV(pipeline, parameters, cv=5,n_jobs=-1, verbose=1)
search.fit(X_train, y_train)
print("Best parameter (CV score=%0.3f):" % search.best_score_)
print(search.best_params_)

# For the random forest model best model report: the confusion matrix and the accuracy. Which items were most misclassified and how?
from sklearn.metrics import confusion_matrix
from sklearn.metrics import accuracy_score

search_acc = accuracy_score(search.predict(X_test), y_test)
search_cm = confusion_matrix(search.predict(X_test), y_test)
print('\nThe accuracy score of the model is: %s'%search_acc)
print('The confusion matrix of the model is: \n%s'%search_cm)

"""# Wine Data - ANSWERS:

- Print the first 10 lines of the data set.  What are the columns?  What type of data? 10
  - The data contains columns that are metrics of wine
  - Each column is of type float64
- Use a command to retrieve the mean, median, 1st and 3rd quartiles of each column.  What is the mean of 'malic_acid', what is the max of magnesium? 10 
  - The mean of "malic_acid" is 2.336348
  - The max of "magnesium" is 162.00
- Plot box plots of all features for each class, which value has the highest median? The most variance? 10
  - "proline" has the highest media and the most variance
- Use RobustScalar to put all features on a similar scale.  Replot the box plots. 10
  - Done
- Do a scatter plot of flavanoids vs hue with each target value being a different color.  How separated is the data? 10
  - The data appears to fit into clusters that border and somewhat overlap each other
  - All the data that has a target of 2 is closest to the origin, whereas the targets of 0 are further from the origin
- Do a NMF decomposition (2 components) of the data.  What features contribute the most to each component? 
  - It appears that 'proline', 'magnesium', 'alcalinity_of_ash', and 'alcohol' contribute the most to these components
- Create a scatter plot of components 1 vs 2 with each target value a different color.  How separated is the data?  10
  - The targets of 1 and 2 are overlapping and visually nearly indistinguishable from one another
  - The target of 0 is mostly separated from the other two targets
- With original data, create a pipeline to scale the data and train a random forest model.  Use GridSearch to find the best model, try 10, 50 and 100 trees and both gini and entropy. 10
  - Optimal values found:
    - criterion = 'entropy'
    - n_estimators = '100'
- For the random forest model best model report: the confusion matrix and the accuracy. Which items were most misclassified and how? 10
  - There was only one entry that was misclassified
  - This was a target of 2 that was misclassified as a 1
  - This is reasonable, since the scatterplots show overlap between targets 1 and 2

# Digits Data:

- Perform a PCA decomposition with 30 components. 10
- How much of the variance is explaned in the 30 components?
- Attempt KMeans and DBSCAN to separate the data. 10
- Report the accuracy of the clusters of both (using the known target data). 10
- Break the data into an 80/20 training test split. 10
- Train a KNN model with neighbors = 3. 10 
- Report confusion matrix and accuracy of the model. Which numbers are most misclassified? What is it most misclassified as? 10
"""

# Perform a PCA decomposition with 30 components.
digits_data = datasets.load_digits()

X = digits_data.data
y = digits_data.target

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.5, random_state=2)

from sklearn.decomposition import PCA

PCA = PCA(n_components=30)
W = PCA.fit_transform(X)
H = PCA.components_

w = pd.DataFrame(data=W)
w['target'] = y

# Attempt KMeans and DBSCAN to separate the data (using the PCA decomposition).
# Report the accuracy of the clusters of both (using the known target data).
from sklearn.cluster import KMeans
from sklearn.cluster import DBSCAN
from sklearn.metrics import accuracy_score
from sklearn.metrics import confusion_matrix

X = w.drop('target',axis=1)
y = w['target']

kmeans = KMeans(n_clusters=2)
kmeans.fit(X)
y_kmeans = kmeans.predict(X)

kmeans_acc = accuracy_score(kmeans.predict(X), y)
kmeans_cm = confusion_matrix(kmeans.predict(X), y)
print('\nThe accuracy score of the KMeans model is: %s'%kmeans_acc)
print('The confusion matrix of the KMeans model is: \n%s'%kmeans_cm)

dbs_acc = accuracy_score(dbs.fit_predict(X), y)
dbs_cm = confusion_matrix(dbs.fit_predict(X), y)
print('\nThe accuracy score of the DBSCAN model is: %s'%dbs_acc)
print('The confusion matrix of the DBSCAN model is: \n%s'%dbs_cm)

# Break the data into an 80/20 training test split.
w = pd.DataFrame(data=W)
w['target'] = y

df_train, df_test = train_test_split(w, test_size=0.2)

# Train a KNN model with neighbors = 3.
from sklearn.neighbors import KNeighborsClassifier as KNN
from sklearn.metrics import accuracy_score
from sklearn.metrics import confusion_matrix

X_train = df_train.drop('target',axis=1)
y_train = df_train['target']
X_test = df_test.drop('target',axis=1)
y_test = df_test['target']


knn = KNN(n_neighbors=3)
knn.fit(X_train, y_train)

# Report confusion matrix and accuracy of the model. Which numbers are most misclassified? What is it most misclassified as?
knn_acc = accuracy_score(knn.predict(X_test), y_test)
knn_cm = confusion_matrix(knn.predict(X_test), y_test)
print('\nThe accuracy score of the model is: %s'%knn_acc)
print('The confusion matrix of the model is: \n%s'%knn_cm)

"""Digits Data:

- Perform a PCA decomposition with 30 components. 10
  - Done
- How much of the variance is explaned in the 30 components?
  - Using the R squared value as a metric for variance
  - The R squared score says that the K Nearest Neighbors model with the PCA components can explain 93.45% of the variance
- Attempt KMeans and DBSCAN to separate the data. 10
  - Done
- Report the accuracy of the clusters of both (using the known target data). 10
  - The KMeans and DBSCAN clusters both appear to be very inaccurate..
- Break the data into an 80/20 training test split. 10
  - Done
- Train a KNN model with neighbors = 3. 10 
  - Done
- Report confusion matrix and accuracy of the model. Which numbers are most misclassified? What is it most misclassified as? 10
  - The classification performed near perfectly
  - In the latest run, there were only 3 misclassifications:
    - A target of 7 was misclassified as a 3
    - A target of 1 was misclassified as a 8
    - A target of 3 was misclassified as a 9
"""